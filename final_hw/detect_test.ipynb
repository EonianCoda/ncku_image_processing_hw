{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./yolov5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\yolov5\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from yolov5.utils.dataloaders import LoadImages\n",
    "from yolov5.utils.general import non_max_suppression, scale_boxes, xyxy2xywh\n",
    "from yolov5.utils.augmentations import letterbox\n",
    "from yolov5.models.experimental import attempt_load\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class Yolov5_Predictor(object):\n",
    "    def __init__(self, weight_path:str, data:str, img_size=(960, 960), device='cpu') -> None:\n",
    "        self.device = torch.device(device) if isinstance(device, str) else device\n",
    "        self.model = attempt_load(weights=weight_path, device=self.device, inplace=True, fuse=True)\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.stride = max(int(self.model.stride.max()), 32)\n",
    "        self.classes_names = self.model.names\n",
    "    def predict(self, img_path:str):\n",
    "        im0 = cv2.imread(img_path)\n",
    "        # Processing image\n",
    "        im = letterbox(im0, self.img_size, stride=self.stride)[0]  # padded resize\n",
    "        im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "        im = np.ascontiguousarray(im)\n",
    "        im = torch.from_numpy(im).to(self.device)\n",
    "        im = im.unsqueeze(dim=0) # add batch_size dimension\n",
    "        im = im.float()  # uint8 to fp16/32\n",
    "        im /= 255\n",
    "        # Do Predict\n",
    "        pred = self.model(im)\n",
    "\n",
    "        conf_thres = 0.1\n",
    "        iou_thres = 0.6\n",
    "        max_det = 1000\n",
    "        # Process predictions\n",
    "        pred = non_max_suppression(pred, conf_thres, iou_thres, max_det=max_det)\n",
    "        det = pred[0]\n",
    "        if len(det):\n",
    "            bboxs = []\n",
    "            det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                # xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                bboxs.append([int(cls), *xyxy]) # [cls, x1, y1, x2, y2]\n",
    "            return bboxs\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "Model summary: 212 layers, 20861016 parameters, 0 gradients, 47.9 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "img_size = (960, 960)\n",
    "weight_path = './runs/train/[2022-12-29-1549]960_bs16_med_E400/weights/best.pt'\n",
    "data = './data/mydataset.yaml'\n",
    "predictor = Yolov5_Predictor(weight_path, data, img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "start_idx = 5\n",
    "end_idx = start_idx + 3\n",
    "\n",
    "category = predictor.classes_names[2]\n",
    "\n",
    "if category == 'scratch':\n",
    "    expand = 100\n",
    "else:\n",
    "    expand = 80\n",
    "\n",
    "split = 'Train'\n",
    "output_root = '../data/stage2/{}/images/'.format(split)\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "unet_img_root = '../data/unet/{}/images/'.format(split)\n",
    "os.makedirs(unet_img_root, exist_ok=True)\n",
    "unet_mask_root = '../data/unet/{}/masks/'.format(split)\n",
    "os.makedirs(unet_mask_root, exist_ok=True)\n",
    "# for i in range(start_idx, end_idx):\n",
    "for i in tqdm(range(100)):\n",
    "    file_name = '{}_{}.png'.format(category, i)\n",
    "    img_path = '../data/yolov5/{}/images/'.format(split) + file_name\n",
    "    result = predictor.predict(img_path)\n",
    "\n",
    "    mask_path = '../data/yolov5/{}/masks/'.format(split) + file_name\n",
    "    im = cv2.imread(img_path)\n",
    "    mask = cv2.imread(mask_path)\n",
    "    im_h, im_w, c = im.shape\n",
    "    new_mask = np.zeros((im_h, im_w, c), dtype=np.uint8)\n",
    "    ratios = []\n",
    "    total_area = im_h * im_w\n",
    "    for box in result:\n",
    "        cls_idx, x1, y1, x2, y2 = box\n",
    "        ratio = (x2 - x1) / (y2 - y1)\n",
    "        if ratio < 1:\n",
    "            ratio = 1 / ratio\n",
    "        ratios.append(ratio)\n",
    "\n",
    "        area = ((x2 - x1) * (y2 - y1)) / total_area\n",
    "        if area <= 0.015:\n",
    "            cur_expand = expand * 2\n",
    "        else:\n",
    "            cur_expand = expand\n",
    "        x1 = max(int(x1) - cur_expand, 0)\n",
    "        x2 = min(int(x2) + cur_expand, im_w)\n",
    "        y1 = max(int(y1) - cur_expand, 0)\n",
    "        y2 = min(int(y2) + cur_expand, im_h)\n",
    "\n",
    "        # crop = im[int(y1):int(y2), int(x1):int(x2), :]\n",
    "        cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 4, cv2.LINE_AA)\n",
    "        cv2.rectangle(mask, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 4, cv2.LINE_AA)\n",
    "        new_mask[y1 : y2, x1 : x2, ...] = 255\n",
    "\n",
    "    # print(np.mean(ratios))\n",
    "    new_size = (im_h // 6, im_w // 6)\n",
    "    im = cv2.resize(im, new_size)\n",
    "    mask = cv2.resize(mask, new_size)\n",
    "    # new_mask = cv2.resize(new_mask, new_size)\n",
    "    # cv2.imshow('image', cv2.resize(new_mask, new_size))\n",
    "    # cv2.waitKey(0)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(new_mask[...,0], connectivity=8, ltype=cv2.CV_16U)\n",
    "    # print(stats)\n",
    "\n",
    "    if num_labels >= 1:\n",
    "        stats[:,2] += stats[:,0]\n",
    "        stats[:,3] += stats[:,1]\n",
    "        new_im = cv2.imread(img_path)\n",
    "        new_mask = cv2.imread(mask_path, 0)\n",
    "        unique = np.unique(new_mask)\n",
    "        ret, new_mask = cv2.threshold(new_mask, unique[1] - 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        raw_name = file_name.split('.')[0]\n",
    "        for i in range(1, num_labels):\n",
    "            new_name = raw_name + '_{}.png'.format(i - 1)\n",
    "            x1, y1, x2, y2, area = stats[i]\n",
    "            x1 = max(int(x1), 0)\n",
    "            x2 = min(int(x2), im_w)\n",
    "            y1 = max(int(y1), 0)\n",
    "            y2 = min(int(y2), im_h)\n",
    "            cv2.imwrite(join(unet_img_root, new_name), new_im[y1:y2, x1:x2, :])\n",
    "            cv2.imwrite(join(unet_mask_root, new_name), new_mask[y1:y2, x1:x2])\n",
    "\n",
    "    # if num_labels >= 1:\n",
    "    #     stats[:,2] += stats[:,0]\n",
    "    #     stats[:,3] += stats[:,1]\n",
    "        # new_im = cv2.imread(img_path)\n",
    "        # new_mask = cv2.imread(mask_path)\n",
    "        new_mask = cv2.imread(mask_path)\n",
    "        for i in range(1, num_labels):\n",
    "            x1, y1, x2, y2, area = stats[i]\n",
    "            x1 = max(int(x1), 0)\n",
    "            x2 = min(int(x2), im_w)\n",
    "            y1 = max(int(y1), 0)\n",
    "            y2 = min(int(y2), im_h)\n",
    "            cv2.rectangle(new_im, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 4, cv2.LINE_AA)\n",
    "            cv2.rectangle(new_mask, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 4, cv2.LINE_AA)\n",
    "\n",
    "        new_im = cv2.resize(new_im, new_size)\n",
    "\n",
    "        new_mask = cv2.resize(new_mask, new_size)\n",
    "\n",
    "        cv2.imwrite(os.path.join(output_root, file_name), cv2.vconcat([cv2.hconcat([mask, im]), cv2.hconcat([new_im, new_mask])]))\n",
    "    # cv2.imshow('image', cv2.vconcat([cv2.hconcat([mask, im]), cv2.hconcat([new_im, new_mask])]))\n",
    "    # cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       0,        0,     3384,     3330, 10884834],\n",
       "       [     959,      842,      202,      434,    87668],\n",
       "       [     983,     1404,      193,      265,    51145],\n",
       "       [     871,     1866,      611,      413,   245073]], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'../data/yolov5/Val/masks/powder_uncover_1.png'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64403dee5e0958e81c767fc1b20107f740770bd4aea477e569d2bae5927e4c0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
